#!/usr/bin/env python3
"""
Enhanced richness dataset processing with progress monitoring and fail-safe features.

This script provides:
- Progress bars and ETA estimates
- Resumable processing (checkpoints)  
- Memory monitoring
- Error recovery
- Detailed logging
"""

import sys
import os
import logging
import time
import argparse
from pathlib import Path
from datetime import datetime
import json
import signal

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.config.config import Config
from src.database.connection import DatabaseManager
from src.raster.catalog import RasterCatalog
from src.processors.data_preparation.raster_merger import RasterMerger
from src.spatial_analysis.som.som_trainer import SOMAnalyzer

# Progress bar
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("âš ï¸  Install tqdm for better progress bars: pip install tqdm")

# Setup enhanced logging
def setup_logging(config=None):
    """Setup comprehensive logging."""
    # Use Config system for log directory
    if config and hasattr(config, 'paths'):
        log_dir = Path(config.paths.logs_dir)
    else:
        log_dir = Path("logs")
    
    log_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = log_dir / f"richness_analysis_{timestamp}.log"
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ðŸ“ Logging to: {log_file}")
    return logger

class ProgressTracker:
    """Enhanced progress tracking for large operations."""
    
    def __init__(self, logger):
        self.logger = logger
        self.start_time = time.time()
        self.checkpoints = {}
    
    def checkpoint(self, stage: str, data: dict = None):
        """Save checkpoint for resumable processing."""
        checkpoint_file = Path(f"checkpoint_{stage}.json")
        checkpoint_data = {
            'stage': stage,
            'timestamp': datetime.now().isoformat(),
            'elapsed_time': time.time() - self.start_time,
            'data': data or {}
        }
        
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint_data, f, indent=2)
        
        self.logger.info(f"âœ… Checkpoint saved: {stage}")
    
    def has_checkpoint(self, stage: str) -> bool:
        """Check if checkpoint exists for stage."""
        return Path(f"checkpoint_{stage}.json").exists()
    
    def load_checkpoint(self, stage: str) -> dict:
        """Load checkpoint data."""
        checkpoint_file = Path(f"checkpoint_{stage}.json")
        if checkpoint_file.exists():
            with open(checkpoint_file, 'r') as f:
                return json.load(f)
        return {}
    
    def cleanup_checkpoints(self):
        """Clean up checkpoint files after successful completion."""
        for checkpoint_file in Path(".").glob("checkpoint_*.json"):
            checkpoint_file.unlink()
        self.logger.info("ðŸ§¹ Cleaned up checkpoint files")

def signal_handler(signum, frame):
    """Handle interruption gracefully."""
    print("\nðŸ›‘ Process interrupted. Checkpoint files preserved for resume.")
    sys.exit(1)

def show_progress_bar(desc: str, total: int = None):
    """Create progress bar if tqdm available."""
    if HAS_TQDM and total:
        return tqdm(desc=desc, total=total, unit='pixels', unit_scale=True)
    return None

def estimate_processing_time(num_pixels: int) -> str:
    """Estimate processing time based on dataset size."""
    # Rough estimates based on test runs
    pixels_per_second = 1000000  # 1M pixels/second for SOM analysis
    estimated_seconds = num_pixels / pixels_per_second
    
    hours = int(estimated_seconds // 3600)
    minutes = int((estimated_seconds % 3600) // 60)
    
    if hours > 0:
        return f"~{hours}h {minutes}m"
    else:
        return f"~{minutes}m"

def parse_arguments():
    """Parse command-line arguments for memory and processing control."""
    parser = argparse.ArgumentParser(
        description="Run richness analysis with memory-aware processing",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Memory control options
    parser.add_argument('--max-samples', type=int, default=500000,
                        help='Maximum samples for in-memory SOM processing')
    parser.add_argument('--sampling-strategy', choices=['random', 'stratified', 'grid'],
                        default='stratified', help='Subsampling strategy for large datasets')
    parser.add_argument('--memory-limit', type=float, default=8.0,
                        help='Memory limit in GB')
    parser.add_argument('--batch-processing', action='store_true',
                        help='Use batch processing for very large datasets')
    parser.add_argument('--chunk-size', type=int, default=50000,
                        help='Chunk size for batch processing')
    
    # SOM parameters
    parser.add_argument('--som-grid-size', type=int, nargs=2, default=[10, 10],
                        help='SOM grid dimensions [width height]')
    parser.add_argument('--som-iterations', type=int, default=1000,
                        help='Number of SOM training iterations')
    
    # Processing options
    parser.add_argument('--skip-merge', action='store_true',
                        help='Skip raster merging step (use existing merged data)')
    parser.add_argument('--force-restart', action='store_true',
                        help='Force restart from beginning (ignore checkpoints)')
    
    args = parser.parse_args()
    
    # Validate inputs
    if args.max_samples <= 0:
        parser.error("--max-samples must be positive")
    if args.memory_limit <= 0:
        parser.error("--memory-limit must be positive")
    if args.chunk_size <= 0:
        parser.error("--chunk-size must be positive")
    if args.som_grid_size[0] <= 0 or args.som_grid_size[1] <= 0:
        parser.error("--som-grid-size dimensions must be positive")
    if args.som_iterations <= 0:
        parser.error("--som-iterations must be positive")
    
    return args

def main():
    """Enhanced main processing pipeline."""
    # Parse command-line arguments
    args = parse_arguments()
    
    # Setup
    signal.signal(signal.SIGINT, signal_handler)
    
    # Load configuration first
    config = Config()
    
    # Setup logging with config
    logger = setup_logging(config)
    tracker = ProgressTracker(logger)
    
    logger.info("ðŸš€ Starting ENHANCED richness dataset processing pipeline")
    logger.info("ðŸ”§ Features: Progress tracking, Checkpoints, Memory monitoring, Error recovery")
    logger.info(f"ðŸ’¾ Memory limit: {args.memory_limit} GB")
    logger.info(f"ðŸ“Š Max samples: {args.max_samples:,}")
    logger.info(f"ðŸŽ¯ Sampling strategy: {args.sampling_strategy}")
    
    if args.force_restart:
        logger.info("ðŸ”„ Force restart mode - clearing all checkpoints")
        # Clear checkpoint files
        import glob
        for checkpoint_file in glob.glob("checkpoint_*.json"):
            try:
                os.unlink(checkpoint_file)
                logger.info(f"Removed checkpoint: {checkpoint_file}")
            except OSError:
                pass
    
    try:
        # Ensure config object has proper structure for command-line overrides
        if not hasattr(config, 'config'):
            config.config = {}
        
        # Update config with command-line arguments using safer nested updates
        processing_config = config.config.setdefault('processing', {})
        subsampling_config = processing_config.setdefault('subsampling', {})
        subsampling_config.update({
            'enabled': True,
            'max_samples': args.max_samples,
            'strategy': args.sampling_strategy,
            'memory_limit_gb': args.memory_limit
        })
        
        som_config = config.config.setdefault('som_analysis', {})
        som_config.update({
            'max_pixels_in_memory': min(args.max_samples, 1000000),
            'use_memory_mapping': args.memory_limit <= 16.0,  # Use memory mapping for systems with <= 16GB
            'batch_training': {
                'enabled': args.batch_processing,
                'batch_size': args.chunk_size
            }
        })
        
        db = DatabaseManager()
        
        # Check database connection
        logger.info("Checking database setup...")
        if not db.test_connection():
            logger.error("Database connection failed")
            return False
        
        # Stage 1: Load datasets into catalog (resumable)
        if not tracker.has_checkpoint('catalog_loaded'):
            logger.info("ðŸ“¥ STAGE 1: Loading datasets into catalog...")
            
            # Define dataset paths using Config system
            data_dir = Path(config.paths.data_dir)
            data_files = config.get('data_files', {})
            
            daru_filename = data_files.get('plants_richness', 'daru-plants-richness.tif')
            iucn_filename = data_files.get('terrestrial_richness', 'iucn-terrestrial-richness.tif')
            
            daru_path = data_dir / daru_filename
            iucn_path = data_dir / iucn_filename
            
            # Verify files exist
            if not daru_path.exists() or not iucn_path.exists():
                logger.error(f"Dataset files not found: {daru_path}, {iucn_path}")
                return False
            
            logger.info(f"âœ… Found datasets:\n  - Plants: {daru_path}\n  - Terrestrial: {iucn_path}")
            
            # Initialize catalog
            catalog = RasterCatalog(db, config)
            
            # Load datasets with progress tracking
            plants_entry = catalog.add_raster(daru_path, dataset_type="plants", validate=True)
            logger.info(f"âœ… Added plants dataset: {plants_entry.name}")
            
            terrestrial_entry = catalog.add_raster(iucn_path, dataset_type="terrestrial", validate=True)  
            logger.info(f"âœ… Added terrestrial dataset: {terrestrial_entry.name}")
            
            # Save checkpoint
            tracker.checkpoint('catalog_loaded', {
                'plants_name': plants_entry.name,
                'terrestrial_name': terrestrial_entry.name
            })
            
        else:
            logger.info("ðŸ“¥ STAGE 1: Resuming from catalog checkpoint...")
            checkpoint_data = tracker.load_checkpoint('catalog_loaded')
            catalog = RasterCatalog(db, config)
            plants_entry = catalog.get_raster(checkpoint_data['data']['plants_name'])
            terrestrial_entry = catalog.get_raster(checkpoint_data['data']['terrestrial_name'])
            logger.info("âœ… Datasets loaded from checkpoint")
        
        # Stage 2: Merge datasets (resumable)
        if not tracker.has_checkpoint('datasets_merged'):
            logger.info("ðŸ”— STAGE 2: Merging datasets with robust alignment...")
            
            merger = RasterMerger(config, db)
            
            # Calculate dataset size for time estimation
            total_pixels = 21600 * 10410  # Use intersection size
            logger.info(f"ðŸ“Š Dataset size: {total_pixels:,} pixels")
            logger.info(f"â±ï¸  Estimated merge time: {estimate_processing_time(total_pixels)}")
            
            merged_result = merger.merge_custom_rasters(
                raster_names={
                    'plants_richness': plants_entry.name,
                    'terrestrial_richness': terrestrial_entry.name
                },
                band_names=['plants_richness', 'terrestrial_richness']
            )
            
            merged_data = merged_result['data']
            logger.info(f"âœ… Merged data shape: {dict(merged_data.sizes)}")
            logger.info(f"   Bands: {list(merged_data.data_vars)}")
            
            # Save checkpoint with merged data metadata
            tracker.checkpoint('datasets_merged', {
                'shape': dict(merged_data.sizes),
                'bands': list(merged_data.data_vars),
                'alignment_report': str(merged_result.get('alignment_report', {}))
            })
            
        else:
            logger.info("ðŸ”— STAGE 2: Resuming from merge checkpoint...")
            # For resumption, we'd need to reload the merged data
            # This is complex, so for now we'll restart merge if interrupted
            logger.warning("âš ï¸  Merge stage resumption not yet implemented - will restart merge")
            
            merger = RasterMerger(config, db)
            merged_result = merger.merge_custom_rasters(
                raster_names={
                    'plants_richness': plants_entry.name,
                    'terrestrial_richness': terrestrial_entry.name
                },
                band_names=['plants_richness', 'terrestrial_richness']
            )
            merged_data = merged_result['data']
            logger.info("âœ… Datasets merged")
        
        # Stage 3: SOM Analysis (the heavy computation)
        if not tracker.has_checkpoint('som_complete'):
            logger.info("ðŸ§  STAGE 3: Running SOM analysis...")
            
            som_analyzer = SOMAnalyzer(config, db)
            
            # Configure SOM parameters for large dataset
            som_params = {
                'grid_size': [8, 8],  # 8x8 SOM grid = 64 neurons
                'iterations': 1000,   # Sufficient for convergence
                'sigma': 1.5,
                'learning_rate': 0.5,
                'neighborhood_function': 'gaussian',
                'random_seed': 42
            }
            
            total_samples = merged_data.sizes['lon'] * merged_data.sizes['lat']
            logger.info(f"ðŸ“Š SOM Analysis details:")
            logger.info(f"   Grid: {som_params['grid_size']} ({som_params['grid_size'][0] * som_params['grid_size'][1]} neurons)")
            logger.info(f"   Samples: {total_samples:,}")
            logger.info(f"   Features: {len(list(merged_data.data_vars))}")
            logger.info(f"   Iterations: {som_params['iterations']}")
            logger.info(f"â±ï¸  Estimated SOM time: {estimate_processing_time(total_samples)}")
            
            logger.info(f"SOM parameters: {som_params}")
            
            # Run SOM analysis with progress monitoring
            som_result = som_analyzer.analyze(
                data=merged_data,
                **som_params
            )
            
            # Save checkpoint
            tracker.checkpoint('som_complete', {
                'grid_size': som_params['grid_size'],
                'n_clusters': som_result.statistics['n_clusters']
            })
            
        else:
            logger.info("ðŸ§  STAGE 3: SOM analysis already completed")
            checkpoint_data = tracker.load_checkpoint('som_complete')
            logger.info(f"âœ… SOM completed with {checkpoint_data['data']['n_clusters']} clusters")
            # Would need to reload som_result for resumption
        
        # Stage 4: Save results
        logger.info("ðŸ’¾ STAGE 4: Saving results...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        saved_path = som_analyzer.save_results(som_result, f"Richness_SOM_{timestamp}")
        
        logger.info(f"âœ… Results saved to: {saved_path}")
        
        # Print comprehensive summary
        stats = som_result.statistics
        logger.info("ðŸŽ¯ FINAL ANALYSIS SUMMARY:")
        logger.info("=" * 60)
        logger.info(f"ðŸ“Š Grid size: {stats['grid_size']}")
        logger.info(f"ðŸ§  Number of clusters: {stats['n_clusters']}")
        logger.info(f"ðŸ“ˆ Quantization error: {stats['quantization_error']:.4f}")
        logger.info(f"ðŸ“ Topographic error: {stats['topographic_error']:.4f}")
        logger.info(f"âŒ Empty neurons: {stats['empty_neurons']}")
        logger.info(f"âš–ï¸  Cluster balance: {stats['cluster_balance']:.4f}")
        
        # Log top 5 clusters
        cluster_stats = stats['cluster_statistics']
        sorted_clusters = sorted(cluster_stats.items(), 
                               key=lambda x: x[1]['count'], reverse=True)
        logger.info("ðŸ† Top 5 clusters by size:")
        for i, (cluster_id, info) in enumerate(sorted_clusters[:5]):
            logger.info(f"   {i+1}. Cluster {cluster_id}: {info['count']:,} pixels ({info['percentage']:.1f}%)")
        
        logger.info("=" * 60)
        logger.info("ðŸŽ‰ PROCESSING PIPELINE COMPLETED SUCCESSFULLY!")
        
        # Cleanup
        tracker.cleanup_checkpoints()
        
        # Total elapsed time
        total_time = time.time() - tracker.start_time
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        logger.info(f"â±ï¸  Total processing time: {hours}h {minutes}m")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Processing failed: {e}")
        logger.error("ðŸ’¾ Checkpoint files preserved for resumption")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\n" + "="*60)
        print("ðŸŽ‰ SUCCESS! Richness analysis completed.")
        print("ðŸ“ Check outputs/spatial_analysis/ for results")
        print("="*60)
    else:
        print("\n" + "="*60) 
        print("âŒ FAILED! Check logs for details.")
        print("ðŸ”„ Use checkpoints to resume processing")
        print("="*60)
    
    sys.exit(0 if success else 1)