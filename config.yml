# Geo Project Configuration
# Optimized for high-memory cluster with 1TB RAM and 256 cores
# 
# FOR LOCAL TESTING: Comment out cluster paths and uncomment local paths
# FOR CLUSTER: Use cluster paths as-is

# Pipeline control - for production
pipeline:
  allow_skip_stages: false  # Set to true if you want to skip already processed stages
  processing_bounds: 'tiny_test'  # Use tiny test region for quick debugging

# =============================================================================
# PATHS CONFIGURATION - ADJUST FOR YOUR ENVIRONMENT
# =============================================================================
# PATHS: Using defaults.py for local testing
# paths:
#   # For cluster deployment, uncomment and modify:
#   data_dir: "/maps/mwd24/richness"     # Cluster shared data storage
#   logs_dir: "/scratch/yl998/geo_logs"  # Scratch space for logs

# NOTE: Individual dataset paths are now specified in each dataset configuration below
# This data_files section is deprecated but kept for backward compatibility

# =============================================================================
# MEMORY OPTIMIZATION - STREAMING EXPORT
# =============================================================================
merge:
  # Enable streaming mode for memory-efficient export
  enable_streaming: true  # Enable for large production datasets
  streaming_chunk_size: 10000  # Larger chunks with available memory
  
  # Regular merge options
  enable_chunked_processing: false
  chunk_size: 5000

# [REMOVED DUPLICATE - see line 314 for export configuration]
  
  # Include metadata file
  include_metadata: true

# =============================================================================
# RESAMPLING CONFIGURATION - NEW UNIFIED PIPELINE
# =============================================================================
resampling:
  # Target resolution for all datasets (in degrees for geographic data)
  target_resolution: 0.016666666666667  # Match coarsest resolution (plants/terrestrial) to avoid upsampling
  target_crs: 'EPSG:4326'
  
  # Resampling strategy per data type
  strategies:
    richness_data: 'sum'          # Sum for count data (using existing SumAggregationStrategy)
    continuous_data: 'average'    # Average for continuous SDM predictions (area-weighted)
    categorical_data: 'majority'  # Majority for categories
  
  # Processing options
  chunk_size: 1000
  validate_output: true
  preserve_sum: true  # Important for richness data
  cache_resampled: true
  engine: 'gdal'   # 'numpy' or 'gdal' - GDAL is faster for large rasters
  
  # Window processing settings (always enabled)
  window_size: 2048  # Process in 2048x2048 windows
  window_overlap: 128  # Overlap for avoiding edge artifacts
  skip_data_loading_for_passthrough: true  # Don't load data for passthrough
  
  # Resolution matching
  allow_skip_resampling: true  # Skip resampling if resolution matches
  resolution_tolerance: 0.001  # Tolerance for resolution matching
  force_passthrough_for_skip: true  # Use passthrough strategy when skipping
  

# =============================================================================
# STORAGE CONFIGURATION - Memory-efficient database operations
# =============================================================================
storage:
  chunk_size: 100000       # 100k pixels threshold for chunked storage (lower for fungi datasets)
  chunk_rows: 500          # Process 500 rows at a time (smaller chunks for memory efficiency)
  aggregate_to_grid: true  # ENABLE for fungi datasets to reduce 233M pixels to ~21M grid cells
  grid_cell_size: 0.05     # 0.05° grid cells (≈ 5.5km) for finer aggregation
  batch_insert_size: 5000  # Smaller batches for stability
  enable_progress_logging: true
  memory_cleanup_interval: 5  # More frequent cleanup

# Multiple dataset definitions for unified processing
# Each dataset now has its own path - no central data_dir needed!
datasets:
  target_datasets:
    # PRODUCTION RASTERS - 4 datasets as requested
    - name: "plants-richness"
      path: "/maps/mwd24/richness/daru-plants-richness.tif"
      data_type: "richness_data"
      band_name: "plants_richness"
      enabled: true
      
    - name: "terrestrial-richness" 
      path: "/maps/mwd24/richness/iucn-terrestrial-richness.tif"
      data_type: "richness_data"
      band_name: "terrestrial_richness"
      enabled: true
      
    - name: "am-fungi-richness"
      path: "/scratch/yl998/resampled_rasters/AM_Fungi_Richness_Predicted_resampled_0.016667.tif"
      data_type: "continuous_data"  # Changed from richness_data - SDM predictions
      band_name: "am_fungi_richness"
      enabled: true
      skip_resampling: true  # Already at target resolution
      
    - name: "ecm-fungi-richness"
      path: "/scratch/yl998/resampled_rasters/EcM_Fungi_Richness_Predicted_resampled_0.016667.tif"
      data_type: "continuous_data"  # Changed from richness_data - SDM predictions
      band_name: "ecm_fungi_richness"
      enabled: true
      skip_resampling: true  # Already at target resolution
      
    # EXAMPLE: Add more datasets when ready:
    # - name: "marine-richness"
    #   path: "/maps/mwd24/marine/marine-richness.tif"  # Different directory
    #   data_type: "richness_data"
    #   band_name: "marine_richness"
    #   enabled: true
      
    # - name: "freshwater-richness"
    #   path: "/scratch/yl998/freshwater/freshwater-richness.tif"  # Another location
    #   data_type: "richness_data" 
    #   band_name: "freshwater_richness"
    #   enabled: true
      
    # SUPPORTS ANY NUMBER OF DATASETS:
    # - name: "soil-biodiversity"
    #   path: "/data/external/soil-biodiversity.tif"
    #   data_type: "richness_data"
    #   band_name: "soil_biodiversity" 
    #   enabled: true
    #
    # - name: "climate-zones"
    #   path: "/maps/climate/climate-zones.tif"
    #   data_type: "categorical_data"  # Different data type
    #   band_name: "climate_zones"
    #   enabled: true

# =============================================================================
# HIGH-MEMORY PROCESSING CONFIGURATION (1TB RAM)
# =============================================================================
processing:
  # Memory and chunking settings - optimized for available resources
  memory_limit_gb: 80              # Conservative with only ~118GB available
  chunk_size: 5000                 # Larger chunks for efficiency
  enable_chunking: true
  checkpoint_interval: 5           # More frequent checkpoints
  merge_chunk_size: 2000           # Larger chunks for merge
  lazy_merge_threshold_mb: 1000    # Use lazy merge for large datasets
  
  subsampling:
    enabled: false  # DISABLED for full dataset quality
    max_samples: 500000000  # Higher than your 224M samples
    memory_limit_gb: 180    # Conservative memory limit
    strategy: 'random'
    min_samples_per_class: 1000
    spatial_block_size: 200
  
  # Optimized for 256-core system with current low usage
  max_workers: 150      # Conservative to avoid overload
  batch_size: 5000     # Larger batches for efficiency
  chunk_size: 10000    # Larger chunks for parallel processing

# =============================================================================
# RASTER PROCESSING FOR HIGH-MEMORY SYSTEM
# =============================================================================
raster_processing:
  memory_limit_mb: 80000   # 80GB in MB (adjusted for available memory)
  max_chunk_size_mb: 2000  # 2GB - force chunking for large datasets
  parallel_workers: 150     # Match processing workers
  tile_size: 1024         # Reduced for better processing of high-res fungi datasets
  cache_ttl_days: 30
  
  lazy_loading:
    chunk_size_mb: 2000   # Large chunks for efficiency
    prefetch_tiles: 5     # Moderate prefetch
  
  resampling_methods:
    default: 'bilinear'
    categorical: 'nearest'
    continuous: 'bilinear'
  
  compression:
    method: 'lzw'
    level: 6

# =============================================================================
# SOM ANALYSIS - OPTIMIZED FOR QUALITY ON HIGH-MEMORY SYSTEM
# =============================================================================
som_analysis:
  max_pixels_in_memory: 100000000  # Reduced for available memory
  memory_overhead_factor: 2.0      # Reduced overhead with abundant RAM
  use_memory_mapping: false        # Keep everything in RAM for speed
  
  # High-quality SOM settings optimized for biodiversity data
  default_grid_size: [12, 12]      # Larger grid for better resolution
  iterations: 8000                 # More iterations for convergence
  sigma: 1.5                       # Standard deviation for neighborhood
  learning_rate: 0.5               # Learning rate
  neighborhood_function: 'gaussian'
  random_seed: 42
  # Note: Manhattan distance is always used (objectively better for biodiversity data)
  
  # Dynamic convergence settings
  enable_dynamic_convergence: true
  convergence_method: 'unified'    # 'unified', 'multi_criteria', 'batch_unified'
  convergence:
    # Weight stabilization thresholds
    weight_threshold: 1e-6
    weight_stabilization_threshold: 1e-6
    weight_stabilization_patience: 3
    
    # Quality thresholds
    quantization_error_threshold: 1e-6
    topographic_error_threshold: 0.01
    convergence_threshold: 0.95     # Unified convergence index threshold
    confidence_level: 0.95
    
    # Unified convergence index weights
    alpha: 0.6  # Weight for embedding accuracy
    beta: 0.4   # Weight for topographic accuracy
    
    # Convergence checking parameters
    check_interval: 100
    min_iterations: 500
    patience: 5
    convergence_window: 10
    
    # Advanced convergence options optimized for biodiversity data
    # Note: Manhattan distance is always used (matches SOM training)
    learning_rate_schedule: 'vlrsom'  # 'fixed', 'linear_decay', 'exponential_decay', 'vlrsom', 'adaptive'
    require_both_criteria: true     # For batch_unified: require both weight stabilization AND quality
  
  # Spatial validation framework for rigorous biodiversity analysis
  validation:
    enabled: true                   # Enable spatial train/validate/test framework
    spatial_split_strategy: 'latitudinal'  # 'latitudinal', 'longitudinal', 'continental', 'ecoregion', 'random_spatial', 'checkerboard'
    train_ratio: 0.7               # 70% for training
    validation_ratio: 0.15         # 15% for validation (early stopping)
    test_ratio: 0.15               # 15% for final evaluation
    patience: 10                   # Epochs without improvement before early stopping
    min_improvement: 1e-5          # Minimum improvement to reset patience
    save_checkpoints: true         # Save best model checkpoints
  
  subsample_ratio: 1.0             # Use full dataset when possible
  min_samples: 50000               # Minimum samples even for small datasets
  
  batch_training:
    enabled: false                 # Disabled with full memory available
    batch_size: 100000
    overlap_ratio: 0.1

# =============================================================================
# PIPELINE QUALITY THRESHOLDS - Adjusted for biodiversity data
# =============================================================================
pipeline:
  quality_thresholds:
    # Biodiversity data is naturally sparse - species have limited ranges
    max_nan_ratio: 0.90              # Allow up to 90% NaN for ecological data
    min_completeness: 0.10           # Only need 10% data coverage
    max_outlier_ratio: 0.05          # 5% outliers acceptable
    min_coverage: 0.50               # 50% spatial coverage is fine

# =============================================================================
# DATABASE CONFIGURATION - ADJUST FOR YOUR ENVIRONMENT
# =============================================================================
database:
  host: 'localhost'
  port: 51051  # Cluster database port
  database: 'geo_cluster_db'       # Cluster database name
  user: 'jason'                    # Cluster username
  password: '123456'        # Cluster password
  
  max_connections: 20               # More connections for parallel processing
  connection_timeout: 30
  retry_attempts: 3

# Database schema mapping for flexibility
database_schema_mapping:
  raster_sources:
    geometry_column: 'spatial_extent'
    fallback_geometry_columns: ['bounds', 'geometry', 'geom', 'shape']
    active_column: 'active'
    status_column: 'processing_status'
    metadata_column: 'metadata'
  grid_cells:
    geometry_column: 'geometry'
    fallback_geometry_columns: ['geom', 'shape', 'bounds']
    active_column: null
    metadata_column: null

# =============================================================================
# GRID SYSTEMS CONFIGURATION
# =============================================================================
grids:
  cubic:
    resolutions: [1000, 5000, 10000, 25000]  # meters - added larger resolutions
    crs: 'EPSG:3857'
    default_resolution: 5000
  hexagonal:
    resolutions: [6, 7, 8, 9]  # H3 levels - added finer resolution
    crs: 'EPSG:4326'
    default_resolution: 8

# =============================================================================
# WORKING DIRECTORIES - ADJUST FOR YOUR CLUSTER
# =============================================================================
output_paths:
  working_dir: "/scratch/yl998/geo_working"
  results_dir: "/scratch/yl998/geo_results"
  temp_dir: "/scratch/yl998/geo_temp"

# =============================================================================
# PROCESSING BOUNDS (GEOGRAPHIC REGIONS)
# =============================================================================
processing_bounds:
  # TINY TEST REGION FOR DEBUGGING
  tiny_test: [-1, -1, 1, 1]  # 2x2 degree box for quick testing
  test_small: [-10, -10, 10, 10]  # 20x20 degree box for quick testing
  global: [-180, -90, 180, 90]
  europe: [-25.0, 35.0, 50.0, 75.0]
  north_america: [-170.0, 15.0, -50.0, 75.0]
  south_america: [-85.0, -60.0, -30.0, 15.0]
  africa: [-25.0, -40.0, 55.0, 40.0]
  asia: [60.0, -15.0, 180.0, 75.0]
  oceania: [110.0, -50.0, 180.0, -10.0]

# =============================================================================
# SPECIES AND FEATURE CONFIGURATION
# =============================================================================
species_filters:
  min_occurrence_count: 5
  exclude_uncertain_coordinates: true
  coordinate_precision_threshold: 0.01
  exclude_cultivated: true
  exclude_fossil: false
  max_year: 2024
  exclude_future_dates: true

features:
  climate_variables: ['bio_1', 'bio_12']  # Temperature, precipitation
  richness_types: ['present', 'absent', 'fossil']

# =============================================================================
# MERGE STAGE CONFIGURATION
# =============================================================================
merge:
  enable_chunked_processing: true
  chunk_size: 5000  # rows per chunk
  enable_validation: true
  alignment_tolerance: 0.01  # degrees

# =============================================================================
# EXPORT STAGE CONFIGURATION
# =============================================================================
export:
  formats: ['parquet']  # Only parquet as requested
  compress: true
  chunk_size: 50000  # Larger chunks for efficiency
  include_metadata: true
  enable_streaming: true  # Streaming for memory efficiency

# =============================================================================
# MONITORING AND LOGGING CONFIGURATION
# =============================================================================
monitoring:
  enable_database_logging: true
  log_batch_size: 100
  log_flush_interval: 5
  enable_metrics: true
  metrics_interval: 10

# =============================================================================
# OUTPUT AND LOGGING CONFIGURATION
# =============================================================================
output_formats:
  csv: true
  parquet: true
  geojson: false

logging:
  level: 'INFO'
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  # file path will use logs_dir from paths section

# =============================================================================
# DATA PREPARATION AND CLEANING
# =============================================================================
data_preparation:
  validation:
    check_crs: true
    check_bounds: false  # Disabled - fungi datasets have slight bounds overflow
    check_nodata: true
    check_dtypes: true
    bounds_tolerance: 0.01  # Allow 0.01 degree tolerance for bounds
  
  cleaning:
    remove_duplicates: true
    handle_missing_values: true
    validate_geometries: true

data_cleaning:
  outlier_detection:
    method: 'iqr'
    threshold: 1.5
  
  coordinate_cleaning:
    precision_threshold: 0.001
    remove_zero_coordinates: true
    validate_country_boundaries: false

# =============================================================================
# TESTING CONFIGURATION
# =============================================================================
testing:
  use_test_database: true
  test_database_name: 'geo_test_db'
  cleanup_after_tests: true
  small_dataset_size: 1000
  integration_test_timeout: 300

# =============================================================================
# PIPELINE CONTROL CONFIGURATION
# =============================================================================
# Moved to top of file to avoid conflicts
# pipeline:
#   # Master switch for stage skipping - USE WITH CAUTION!
#   allow_skip_stages: true  # Set to true only when you're sure data is fresh
#   
#   # Stage-specific skip settings
#   stages:
#     data_load:
#       skip_if_exists: true
#     resample:
#       skip_if_exists: true  # Set to true to use existing DB data
#       
#   # Data validation for skip decisions
#   data_validation:
#     max_age_hours: 24  # Consider data stale after this
#     check_source_timestamps: true  # Verify source files haven't changed
#     
#   # Other pipeline settings
#   cleanup_checkpoints_on_success: true
#   checkpoint_interval_stages: 1
#   lazy_merge_threshold_mb: 500

# =============================================================================
# MACHINE LEARNING CONFIGURATION
# =============================================================================
machine_learning:
  # Default settings for all ML runs
  defaults:
    model_type: 'linear_regression'
    target_column: 'total_richness'
    perform_cv: true
    save_model: true
    save_predictions: true
    cv_strategy:
      type: 'spatial_block'
      n_splits: 5
      block_size: 100
    imputation_strategy:
      type: 'spatial_knn'
      n_neighbors: 10
      spatial_weight: 0.6
  
  # Named ML experiments for reproducibility
  experiments:
    # Quick test with linear regression
    test_linear:
      input_parquet: 'outputs/biodiversity_test.parquet'
      model_type: 'linear_regression'
      cv_strategy:
        type: 'spatial_block'
        n_splits: 3
        block_size: 200
    
    # Test with generated data
    test_generated:
      input_parquet: 'outputs/test_biodiversity.parquet'
      model_type: 'linear_regression'
      target_column: 'total_richness'
      cv_strategy:
        type: 'spatial_block'
        n_splits: 3
        block_size: 100
    
    # Production LightGBM model
    production_lgb:
      input_parquet: 'outputs/biodiversity_global.parquet'
      model_type: 'lightgbm'
      target_column: 'total_richness'
      cv_strategy:
        type: 'spatial_buffer'
        n_splits: 5
        buffer_distance: 50
      feature_columns: null  # auto-detect
    
    # Richness prediction with no CV
    quick_richness:
      input_parquet: 'outputs/biodiversity_latest.parquet'
      model_type: 'linear_regression'
      target_column: 'plants_richness'
      perform_cv: false
      save_predictions: true
    
    # Regional model for specific area
    europe_model:
      input_parquet: 'outputs/biodiversity_europe.parquet'
      model_type: 'lightgbm'
      cv_strategy:
        type: 'environmental'
        n_splits: 5
        stratify_by: 'latitude'
  
  # Research-oriented analysis configuration
  research:
    # Default nested model formulas for biodiversity hypotheses
    default_formulas:
      - 'F ~ avg_temp + avg_precip + seasonal_temp'
      - 'F ~ avg_temp + avg_precip + seasonal_temp + P + A'
      - 'F ~ avg_temp + avg_precip + seasonal_temp + P + A + P:seasonal_temp'
    
    # Permutation importance settings
    permutation_importance:
      n_repeats: 10
      random_state: 42
      model_to_test: 'model_2'  # Test if P and A have importance beyond climate
    
    # Interaction analysis settings
    interaction_analysis:
      primary_feature: 'P'
      secondary_feature: 'seasonal_temp'
      model_to_test: 'model_3'
      grid_resolution: 25
    
    # Research experiments
    experiments:
      # Test the temperate mismatch hypothesis
      temperate_mismatch:
        input_parquet: 'outputs/test_biodiversity.parquet'
        nested_formulas:
          - 'F ~ avg_temp + avg_precip + seasonal_temp'
          - 'F ~ avg_temp + avg_precip + seasonal_temp + P + A'
          - 'F ~ avg_temp + avg_precip + seasonal_temp + P + A + P:seasonal_temp'
        experiment_name: 'temperate_mismatch_test'
      
      # Extended analysis with more climate interactions
      climate_interactions:
        input_parquet: 'outputs/biodiversity_global.parquet'
        nested_formulas:
          - 'F ~ avg_temp + avg_precip'
          - 'F ~ avg_temp + avg_precip + seasonal_temp'
          - 'F ~ avg_temp + avg_precip + seasonal_temp + P + A'
          - 'F ~ avg_temp + avg_precip + seasonal_temp + P + A + P:seasonal_temp + A:avg_temp'
        experiment_name: 'extended_climate_analysis'

# =============================================================================
# SPATIAL ANALYSIS CONFIGURATION
# =============================================================================
spatial_analysis:
  # Default settings for all analysis runs
  defaults:
    data_source: 'parquet'  # parquet|database|csv
    save_results: true
    save_intermediate: false
    keep_results_in_memory: false
    chunk_size: 10000
    memory_limit_gb: 8.0
    enable_checkpointing: true
    checkpoint_interval: 100  # Save checkpoint every N chunks
    
  # Named analysis experiments for reproducibility
  experiments:
    # Quick SOM test with unified convergence
    test_som:
      input_parquet: 'outputs/test_biodiversity.parquet'
      method: 'som'
      grid_size: [6, 6]
      max_iterations: 2000
      convergence_threshold: 1e-6
      enable_dynamic_convergence: true
      convergence_method: 'unified'
      
    # Production SOM analysis with advanced convergence
    production_som:
      input_parquet: 'outputs/biodiversity_global.parquet'
      method: 'som'
      grid_size: [12, 12]
      max_iterations: 8000
      convergence_threshold: 1e-7
      enable_dynamic_convergence: true
      convergence_method: 'unified'
      sigma: 1.5
      learning_rate: 0.5
      
    # Batch SOM test with batch unified convergence
    test_batch_som:
      input_parquet: 'outputs/test_biodiversity.parquet'
      method: 'som'
      grid_size: [8, 8]
      max_iterations: 1000
      enable_dynamic_convergence: true
      convergence_method: 'batch_unified'  # Uses batch training + unified convergence index!
      sigma: 1.0
      learning_rate: 0.3
      
    # Advanced unified convergence test
    test_advanced_som:
      input_parquet: 'outputs/test_biodiversity.parquet'
      method: 'som'
      grid_size: [10, 10]
      max_iterations: 2000
      enable_dynamic_convergence: true
      convergence_method: 'unified'  # Uses VLRSOM + unified convergence index!
      sigma: 1.2
      learning_rate: 0.4
      
    # GWPCA test
    test_gwpca:
      input_parquet: 'outputs/test_biodiversity.parquet'
      method: 'gwpca'
      n_components: 3
      bandwidth: 'adaptive'
      kernel: 'gaussian'
      adaptive_bw: 50
      
    # Production GWPCA
    production_gwpca:
      input_parquet: 'outputs/biodiversity_global.parquet'
      method: 'gwpca'
      n_components: 5
      bandwidth: 'adaptive'
      kernel: 'gaussian'
      adaptive_bw: 100
      
    # MaxP regionalization test
    test_maxp:
      input_parquet: 'outputs/test_biodiversity.parquet'
      method: 'maxp_regions'
      min_region_size: 10
      spatial_weights: 'queen'
      threshold_variable: 'total_richness'
      method_type: 'ward'
      random_seed: 42

# Method-specific configurations (maintains backward compatibility)
som_analysis:
  max_pixels_in_memory: 1000000
  memory_overhead_factor: 3.0
  use_memory_mapping: true
  default_grid_size: [10, 10]
  iterations: 8000
  sigma: 1.5
  learning_rate: 0.5
  neighborhood_function: 'gaussian'
  random_seed: 42
  
  # Dynamic convergence settings
  convergence:
    enable_dynamic: true
    weight_threshold: 1e-6
    quantization_error_threshold: 1e-6
    check_interval: 100  # Check every N iterations
    min_iterations: 500  # Minimum iterations before checking
    patience: 5  # Stop if no improvement for N checks

gwpca_analysis:
  n_components: 3
  bandwidth: 'adaptive'
  kernel: 'gaussian'
  adaptive_bw: 50
  standardize_data: true

maxp_analysis:
  n_regions: 10
  min_region_size: 5
  method: 'ward'
  spatial_weights: 'queen'
  random_seed: 42