# Geo Project Configuration
# Optimized for high-memory cluster with 1TB RAM and 256 cores
# 
# FOR LOCAL TESTING: Comment out cluster paths and uncomment local paths
# FOR CLUSTER: Use cluster paths as-is

# =============================================================================
# PATHS CONFIGURATION - ADJUST FOR YOUR ENVIRONMENT
# =============================================================================
# PATHS: Using defaults.py for local testing
# paths:
#   # For cluster deployment, uncomment and modify:
#   data_dir: "/maps/mwd24/richness"     # Cluster shared data storage
#   logs_dir: "/scratch/yl998/geo_logs"  # Scratch space for logs

# NOTE: Individual dataset paths are now specified in each dataset configuration below
# This data_files section is deprecated but kept for backward compatibility

# =============================================================================
# MEMORY OPTIMIZATION - STREAMING EXPORT
# =============================================================================
merge:
  # Enable streaming mode for memory-efficient export
  enable_streaming: false  # Set to true for large datasets
  streaming_chunk_size: 5000  # Rows per chunk in streaming mode
  
  # Regular merge options
  enable_chunked_processing: false
  chunk_size: 5000

export:
  # Export formats
  formats: ['csv']  # Streaming only supports CSV
  
  # Compression
  compress: false  # Set to true for gzip compression
  
  # Chunk size for writing
  chunk_size: 10000
  
  # Include metadata file
  include_metadata: true

# =============================================================================
# RESAMPLING CONFIGURATION - NEW UNIFIED PIPELINE
# =============================================================================
resampling:
  # Target resolution for all datasets (in degrees for geographic data)
  target_resolution: 0.016667  # ~5km at equator (adjust as needed)
  target_crs: 'EPSG:4326'
  
  # Resampling strategy per data type
  strategies:
    richness_data: 'sum'          # Sum for count data (using existing SumAggregationStrategy)
    continuous_data: 'bilinear'   # Bilinear for continuous data
    categorical_data: 'majority'  # Majority for categories
  
  # Processing options
  chunk_size: 1000
  validate_output: true
  preserve_sum: true  # Important for richness data
  cache_resampled: true
  engine: 'numpy'  # 'numpy' or 'gdal'
  
  # Memory-aware processing settings
  enable_memory_aware_processing: true  # Enable windowed processing
  window_size: 2048  # Process in 2048x2048 windows
  window_overlap: 128  # Overlap for avoiding edge artifacts
  skip_data_loading_for_passthrough: true  # Don't load data for passthrough
  
  # Resolution matching
  allow_skip_resampling: true  # Skip resampling if resolution matches
  resolution_tolerance: 0.001  # Tolerance for resolution matching
  
  # Feature flags for gradual rollout
  use_legacy_passthrough: false  # Set to true to use old behavior
  use_legacy_resampling: false  # Set to true to use old behavior

# Multiple dataset definitions for unified processing
# Each dataset now has its own path - no central data_dir needed!
datasets:
  target_datasets:
    - name: "plants-richness"
      path: "/maps/mwd24/richness/daru-plants-richness.tif"  # Full production data
      data_type: "richness_data"
      band_name: "plants_richness"
      enabled: true
      
    - name: "terrestrial-richness" 
      path: "/maps/mwd24/richness/iucn-terrestrial-richness.tif"  # Full production data
      data_type: "richness_data"
      band_name: "terrestrial_richness"
      enabled: true
      
    # EXAMPLE: Add 4-dataset configuration for cluster:
    - name: "marine-richness"
      path: "/maps/mwd24/marine/marine-richness.tif"  # Different directory
      data_type: "richness_data"
      band_name: "marine_richness"
      enabled: false  # Set to true when ready
      
    - name: "freshwater-richness"
      path: "/scratch/yl998/freshwater/freshwater-richness.tif"  # Another location
      data_type: "richness_data" 
      band_name: "freshwater_richness"
      enabled: false  # Set to true when ready
      
    # SUPPORTS ANY NUMBER OF DATASETS:
    # - name: "soil-biodiversity"
    #   path: "/data/external/soil-biodiversity.tif"
    #   data_type: "richness_data"
    #   band_name: "soil_biodiversity" 
    #   enabled: true
    #
    # - name: "climate-zones"
    #   path: "/maps/climate/climate-zones.tif"
    #   data_type: "categorical_data"  # Different data type
    #   band_name: "climate_zones"
    #   enabled: true

# =============================================================================
# HIGH-MEMORY PROCESSING CONFIGURATION (1TB RAM)
# =============================================================================
processing:
  # Memory and chunking settings
  memory_limit_gb: 800             # Adjust based on your system
  chunk_size: 1000                 # Grid cells per chunk
  enable_chunking: true
  checkpoint_interval: 10          # Checkpoint every N chunks
  merge_chunk_size: 500            # Size of chunks for lazy merge
  lazy_merge_threshold_mb: 100     # Use lazy merge for datasets larger than this (set low for testing)
  
  subsampling:
    enabled: false  # DISABLED for full dataset quality on high-memory system
    max_samples: 500000000  # Higher than your 224M samples
    memory_limit_gb: 750    # Reduced to fit available memory
    strategy: 'random'
    min_samples_per_class: 1000
    spatial_block_size: 200
  
  # Optimized for 256-core system
  max_workers: 200      # Conservative for shared cluster
  batch_size: 1000     # Smaller for testing
  chunk_size: 2000     # Smaller for testing

# =============================================================================
# RASTER PROCESSING FOR HIGH-MEMORY SYSTEM
# =============================================================================
raster_processing:
  memory_limit_mb: 750000  # 750GB in MB
  parallel_workers: 200     # Use multiple cores
  tile_size: 2000         # Larger tiles for efficiency
  cache_ttl_days: 30
  
  lazy_loading:
    chunk_size_mb: 1000   # Large chunks with abundant memory
    prefetch_tiles: 10    # Prefetch more tiles
  
  resampling_methods:
    default: 'bilinear'
    categorical: 'nearest'
    continuous: 'bilinear'
  
  compression:
    method: 'lzw'
    level: 6

# =============================================================================
# SOM ANALYSIS - OPTIMIZED FOR QUALITY ON HIGH-MEMORY SYSTEM
# =============================================================================
som_analysis:
  max_pixels_in_memory: 500000000  # Handle full dataset in memory
  memory_overhead_factor: 2.0      # Reduced overhead with abundant RAM
  use_memory_mapping: false        # Keep everything in RAM for speed
  
  # High-quality SOM settings
  default_grid_size: [12, 12]      # Larger grid for better resolution
  iterations: 8000                 # More iterations for convergence
  sigma: 1.5                       # Standard deviation for neighborhood
  learning_rate: 0.5               # Learning rate
  neighborhood_function: 'gaussian'
  random_seed: 42
  
  subsample_ratio: 1.0             # Use full dataset when possible
  min_samples: 50000               # Minimum samples even for small datasets
  
  batch_training:
    enabled: false                 # Disabled with full memory available
    batch_size: 100000
    overlap_ratio: 0.1

# =============================================================================
# DATABASE CONFIGURATION - ADJUST FOR YOUR ENVIRONMENT
# =============================================================================
database:
  host: 'localhost'
  port: 51051  # Cluster database port
  database: 'geo_cluster_db'       # Cluster database name
  user: 'jason'                    # Cluster username
  password: '123456'        # Cluster password
  
  max_connections: 20               # More connections for parallel processing
  connection_timeout: 30
  retry_attempts: 3

# Database schema mapping for flexibility
database_schema_mapping:
  raster_sources:
    geometry_column: 'spatial_extent'
    fallback_geometry_columns: ['bounds', 'geometry', 'geom', 'shape']
    active_column: 'active'
    status_column: 'processing_status'
    metadata_column: 'metadata'
  grid_cells:
    geometry_column: 'geometry'
    fallback_geometry_columns: ['geom', 'shape', 'bounds']
    active_column: null
    metadata_column: null

# =============================================================================
# GRID SYSTEMS CONFIGURATION
# =============================================================================
grids:
  cubic:
    resolutions: [1000, 5000, 10000, 25000]  # meters - added larger resolutions
    crs: 'EPSG:3857'
    default_resolution: 5000
  hexagonal:
    resolutions: [6, 7, 8, 9]  # H3 levels - added finer resolution
    crs: 'EPSG:4326'
    default_resolution: 8

# =============================================================================
# WORKING DIRECTORIES - ADJUST FOR YOUR CLUSTER
# =============================================================================
output_paths:
  working_dir: "/scratch/yl998/geo_working"
  results_dir: "/scratch/yl998/geo_results"
  temp_dir: "/scratch/yl998/geo_temp"

# =============================================================================
# PROCESSING BOUNDS (GEOGRAPHIC REGIONS)
# =============================================================================
processing_bounds:
  # TINY TEST REGION FOR DEBUGGING
  tiny_test: [-1, -1, 1, 1]  # 2x2 degree box for quick testing
  test_small: [-10, -10, 10, 10]  # 20x20 degree box for quick testing
  global: [-180, -90, 180, 90]
  europe: [-25.0, 35.0, 50.0, 75.0]
  north_america: [-170.0, 15.0, -50.0, 75.0]
  south_america: [-85.0, -60.0, -30.0, 15.0]
  africa: [-25.0, -40.0, 55.0, 40.0]
  asia: [60.0, -15.0, 180.0, 75.0]
  oceania: [110.0, -50.0, 180.0, -10.0]

# =============================================================================
# SPECIES AND FEATURE CONFIGURATION
# =============================================================================
species_filters:
  min_occurrence_count: 5
  exclude_uncertain_coordinates: true
  coordinate_precision_threshold: 0.01
  exclude_cultivated: true
  exclude_fossil: false
  max_year: 2024
  exclude_future_dates: true

features:
  climate_variables: ['bio_1', 'bio_12']  # Temperature, precipitation
  richness_types: ['present', 'absent', 'fossil']

# =============================================================================
# MERGE STAGE CONFIGURATION
# =============================================================================
merge:
  enable_chunked_processing: true
  chunk_size: 5000  # rows per chunk
  enable_validation: true
  alignment_tolerance: 0.01  # degrees

# =============================================================================
# EXPORT STAGE CONFIGURATION
# =============================================================================
export:
  formats: ['csv', 'parquet']  # Export both formats by default
  compress: true
  chunk_size: 10000
  include_metadata: true
  enable_streaming: true  # New option for streaming export

# =============================================================================
# MONITORING AND LOGGING CONFIGURATION
# =============================================================================
monitoring:
  enable_database_logging: true
  log_batch_size: 100
  log_flush_interval: 5
  enable_metrics: true
  metrics_interval: 10

# =============================================================================
# OUTPUT AND LOGGING CONFIGURATION
# =============================================================================
output_formats:
  csv: true
  parquet: true
  geojson: false

logging:
  level: 'INFO'
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  # file path will use logs_dir from paths section

# =============================================================================
# DATA PREPARATION AND CLEANING
# =============================================================================
data_preparation:
  validation:
    check_crs: true
    check_bounds: true
    check_nodata: true
    check_dtypes: true
  
  cleaning:
    remove_duplicates: true
    handle_missing_values: true
    validate_geometries: true

data_cleaning:
  outlier_detection:
    method: 'iqr'
    threshold: 1.5
  
  coordinate_cleaning:
    precision_threshold: 0.001
    remove_zero_coordinates: true
    validate_country_boundaries: false

# =============================================================================
# TESTING CONFIGURATION
# =============================================================================
testing:
  use_test_database: true
  test_database_name: 'geo_test_db'
  cleanup_after_tests: true
  small_dataset_size: 1000
  integration_test_timeout: 300

# =============================================================================
# PIPELINE CONTROL CONFIGURATION
# =============================================================================
pipeline:
  # Master switch for stage skipping - USE WITH CAUTION!
  allow_skip_stages: true  # Set to true only when you're sure data is fresh
  
  # Stage-specific skip settings
  stages:
    data_load:
      skip_if_exists: true
    resample:
      skip_if_exists: true  # Set to true to use existing DB data
      
  # Data validation for skip decisions
  data_validation:
    max_age_hours: 24  # Consider data stale after this
    check_source_timestamps: true  # Verify source files haven't changed
    
  # Other pipeline settings
  cleanup_checkpoints_on_success: true
  checkpoint_interval_stages: 1
  lazy_merge_threshold_mb: 500

# =============================================================================
# MACHINE LEARNING CONFIGURATION
# =============================================================================
machine_learning:
  # Default settings for all ML runs
  defaults:
    model_type: 'linear_regression'
    target_column: 'total_richness'
    perform_cv: true
    save_model: true
    save_predictions: true
    cv_strategy:
      type: 'spatial_block'
      n_splits: 5
      block_size: 100
    imputation_strategy:
      type: 'spatial_knn'
      n_neighbors: 10
      spatial_weight: 0.6
  
  # Named ML experiments for reproducibility
  experiments:
    # Quick test with linear regression
    test_linear:
      input_parquet: 'outputs/biodiversity_test.parquet'
      model_type: 'linear_regression'
      cv_strategy:
        type: 'spatial_block'
        n_splits: 3
        block_size: 200
    
    # Test with generated data
    test_generated:
      input_parquet: 'outputs/test_biodiversity.parquet'
      model_type: 'linear_regression'
      target_column: 'total_richness'
      cv_strategy:
        type: 'spatial_block'
        n_splits: 3
        block_size: 100
    
    # Production LightGBM model
    production_lgb:
      input_parquet: 'outputs/biodiversity_global.parquet'
      model_type: 'lightgbm'
      target_column: 'total_richness'
      cv_strategy:
        type: 'spatial_buffer'
        n_splits: 5
        buffer_distance: 50
      feature_columns: null  # auto-detect
    
    # Richness prediction with no CV
    quick_richness:
      input_parquet: 'outputs/biodiversity_latest.parquet'
      model_type: 'linear_regression'
      target_column: 'plants_richness'
      perform_cv: false
      save_predictions: true
    
    # Regional model for specific area
    europe_model:
      input_parquet: 'outputs/biodiversity_europe.parquet'
      model_type: 'lightgbm'
      cv_strategy:
        type: 'environmental'
        n_splits: 5
        stratify_by: 'latitude'